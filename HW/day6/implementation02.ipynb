{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import requests"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = os.path.expanduser(\"Qwen3-Reranker-0.6B\")\n",
    "\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "reranker_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ").eval()"
   ],
   "id": "ac4cf4847d0b53dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "token_false_id = reranker_tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id = reranker_tokenizer.convert_tokens_to_ids(\"yes\")"
   ],
   "id": "e166596d57454871",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "max_reranker_length = 8192",
   "id": "4817df37cca7c2d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n\""
   ],
   "id": "ac93b4acb2236d80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prefix_tokens = reranker_tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = reranker_tokenizer.encode(suffix, add_special_tokens=False)"
   ],
   "id": "7c633bd56e725eb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Reranker 函數\n",
    "def format_instruction(instruction, query, doc):\n",
    "    \"\"\"格式化 reranker 的輸入\"\"\"\n",
    "    if instruction is None:\n",
    "        instruction = '根據查詢檢索相關文件'\n",
    "\n",
    "    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(\n",
    "        instruction=instruction, query=query, doc=doc\n",
    "    )\n",
    "    return output"
   ],
   "id": "a17619bb213e3d23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_embedding(texts):\n",
    "    # 1. 確保輸入是 list\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    payload = {\n",
    "        \"texts\": texts,\n",
    "        \"normalize\": True,\n",
    "        \"batch_size\": 32\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(EMBEDDING_API_URL, json=payload, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "\n",
    "        return result.get(\"embedding\", [])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"* Embedding failed: {e}\")\n",
    "        return []"
   ],
   "id": "1ebe7f2f6b7f9be8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_inputs(pairs):\n",
    "    processed_pairs = []\n",
    "    for pair in pairs:\n",
    "        pair_ids = reranker_tokenizer.encode(\n",
    "            pair,\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            max_length=max_reranker_length - len(prefix_tokens) - len(suffix_tokens)\n",
    "        )\n",
    "        full_ids = prefix_tokens + pair_ids + suffix_tokens\n",
    "        processed_pairs.append(reranker_tokenizer.decode(full_ids))\n",
    "\n",
    "        inputs = reranker_tokenizer(\n",
    "            processed_pairs,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_reranker_length\n",
    "        )\n",
    "\n",
    "        for key in inputs:\n",
    "            inputs[key] = input[key].to(reranker_model.device)\n",
    "\n",
    "        return  inputs"
   ],
   "id": "3a1efc3216c395f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def compute_logits(inputs):\n",
    "    \"\"\"計算相關性分數\"\"\"\n",
    "    batch_scores = reranker_model(**inputs).logits[:, -1, :]\n",
    "\n",
    "    true_vector = batch_scores[:, token_true_id]\n",
    "    false_vector = batch_scores[:, token_false_id]\n",
    "\n",
    "    batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "    batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "\n",
    "    scores = batch_scores[:, 1].exp().tolist()\n",
    "\n",
    "    return scores"
   ],
   "id": "d826a946a37206d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def rerank_documents(query, documents, task_instruction=None):\n",
    "    \"\"\"\n",
    "    使用 Qwen3-Reranker 重新排序文件\n",
    "\n",
    "    Args:\n",
    "        query: 查詢字串\n",
    "        documents: 文件列表\n",
    "        task_instruction: 任務指令（可選）\n",
    "\n",
    "    Returns:\n",
    "        排序後的 (文件, 分數) 元組列表\n",
    "    \"\"\"\n",
    "    if task_instruction is None:\n",
    "        task_instruction = '根據查詢檢索相關的技術文件'\n",
    "\n",
    "    # 格式化輸入\n",
    "    pairs = [format_instruction(task_instruction, query, doc) for doc in documents]\n",
    "\n",
    "    # 處理輸入並計算分數\n",
    "    inputs = process_inputs(pairs)\n",
    "    scores = compute_logits(inputs)\n",
    "\n",
    "    # 組合文件和分數，並按分數降序排序\n",
    "    doc_scores = list(zip(documents, scores))\n",
    "    doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return doc_scores\n"
   ],
   "id": "67483cf7a9ea6682",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 整合 Reranker 的混合搜索\n",
    "def hybrid_search_with_rerank(query: str, initial_limit: int = 20, final_limit: int = 3):\n",
    "    \"\"\"\n",
    "    使用 RRF 混合搜索 + Reranker 重排\n",
    "\n",
    "    Args:\n",
    "        query: 查詢字串\n",
    "        initial_limit: 初始檢索的文件數量（用於 reranking）\n",
    "        final_limit: 最終返回的文件數量\n",
    "\n",
    "    Returns:\n",
    "        重排後的 top-k 結果\n",
    "    \"\"\"\n",
    "    # 用 API 取得 query 的嵌入向量\n",
    "    query_embedding = get_embeddings([query], task_description=\"檢索技術文件\")[0]\n",
    "\n",
    "    # 混合搜索（RRF）\n",
    "    response = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        prefetch=[\n",
    "            # BM25 關鍵字搜索\n",
    "            models.Prefetch(\n",
    "                query=models.Document(\n",
    "                    text=query,\n",
    "                    model=\"Qdrant/bm25\",\n",
    "                ),\n",
    "                using=\"sparse\",\n",
    "                limit=initial_limit,\n",
    "            ),\n",
    "            # 語義搜索\n",
    "            models.Prefetch(\n",
    "                query=query_embedding,\n",
    "                using=\"dense\",\n",
    "                limit=initial_limit,\n",
    "            ),\n",
    "        ],\n",
    "        # 使用 RRF 融合演算法\n",
    "        query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
    "        limit=initial_limit,\n",
    "    )\n"
   ],
   "id": "ddd89bc34921edd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # 提取候選文件\n",
    "    candidate_docs = [point.payload[\"text\"] for point in response.points]\n",
    "\n",
    "    if not candidate_docs:\n",
    "        return []\n",
    "\n",
    "    # 使用 Reranker 重新排序\n",
    "    print(f\"正在對 {len(candidate_docs)} 個候選文件進行重排...\")\n",
    "    reranked_results = rerank_documents(query, candidate_docs)\n",
    "\n",
    "    # 返回 top-k 結果\n",
    "    top_results = reranked_results[:final_limit]\n",
    "\n",
    "    print(f\"\\n查詢: {query}\")\n",
    "    print(f\"重排後的 Top {final_limit} 結果:\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for i, (doc, score) in enumerate(top_results, 1):\n",
    "        print(f\"\\n[{i}] 相關性分數: {score:.4f}\")\n",
    "        print(f\"文件: {doc}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    return top_results"
   ],
   "id": "817a35fe9b55a78f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 執行混合搜索 + Reranking\n",
    "query = \"如何使用向量資料庫進行語義搜索？\"\n",
    "results = hybrid_search_with_rerank(\n",
    "    query=query,\n",
    "    initial_limit=20, # top-N\n",
    "    final_limit=3 # top-K\n",
    ")\n"
   ],
   "id": "462eacb99438ae25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = os.path.expanduser(\"~/AI/Models/Qwen3-Reranker-0.6B\")\n",
    "\n",
    "\n",
    "# 載入模型\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "\n",
    "reranker_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    local_files_only=True,\n",
    "    trust_remote_code=True\n",
    ").eval()\n",
    "\n",
    "\n",
    "print(\"Reranker model loaded successfully.\")\n",
    "\n",
    "\n",
    "# 獲取 token IDs，先獲取才能知道結果\n",
    "token_false_id = reranker_tokenizer.convert_tokens_to_ids(\"no\")\n",
    "token_true_id = reranker_tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "\n",
    "\n",
    "# 最大長度設定\n",
    "max_reranker_length = 8192\n",
    "\n",
    "\n",
    "# Prompt 模板\n",
    "prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "suffix = \"<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "\n",
    "\n",
    "prefix_tokens = reranker_tokenizer.encode(prefix, add_special_tokens=False)\n",
    "suffix_tokens = reranker_tokenizer.encode(suffix, add_special_tokens=False)\n",
    "\n",
    "\n",
    "print(\"Reranker configuration completed.\")\n"
   ],
   "id": "1ba29176abe6ae5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def format_instruction(instruction, query, doc):\n",
    "    \"\"\"格式化 reranker 的輸入\"\"\"\n",
    "    if instruction is None:\n",
    "        instruction = '根據查詢檢索相關文件'\n",
    "\n",
    "    output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(\n",
    "        instruction=instruction, query=query, doc=doc\n",
    "    )\n",
    "    return output"
   ],
   "id": "7c51037850d7ea81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_inputs(pairs):\n",
    "    \"\"\"處理 reranker 的輸入\"\"\"\n",
    "    # 先加 prefix 和 suffix\n",
    "    processed_pairs = []\n",
    "    for pair in pairs:\n",
    "        pair_ids = reranker_tokenizer.encode(\n",
    "            pair,\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            max_length=max_reranker_length - len(prefix_tokens) - len(suffix_tokens)\n",
    "        )\n",
    "        full_ids = prefix_tokens + pair_ids + suffix_tokens\n",
    "        processed_pairs.append(reranker_tokenizer.decode(full_ids))\n",
    "\n",
    "    # 一次就完成編碼和填充\n",
    "    inputs = reranker_tokenizer(\n",
    "        processed_pairs,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_reranker_length\n",
    "    )\n",
    "\n",
    "    # 移動到模型設備\n",
    "    for key in inputs:\n",
    "        inputs[key] = inputs[key].to(reranker_model.device)\n",
    "\n",
    "    return inputs\n"
   ],
   "id": "46f404c5388b69c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "\n",
    "\n",
    "class LlamaCppModel(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_url=\"https://ws-02.wade0426.me/v1\",\n",
    "        model_name=\"local-model\"\n",
    "    ):\n",
    "        self.base_url = base_url\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def load_model(self):\n",
    "        # 建立 OpenAI 客戶端\n",
    "        return OpenAI(\n",
    "            api_key=\"NoNeed\",\n",
    "            base_url=self.base_url\n",
    "        )\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        client = self.load_model()\n",
    "        response = client.chat.completions.create(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        # 如果需要非同步版本，可以使用 AsyncOpenAI\n",
    "        # 這裡為簡化示範，直接重用同步方法\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return f\"Llama.cpp ({self.model_name})\"\n"
   ],
   "id": "d3bcf0532b9799c0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "\n",
    "# 初始化自訂模型\n",
    "custom_llm = LlamaCppModel(\n",
    "    base_url=\"https://ws-02.wade0426.me/v1\",\n",
    "    model_name=\"your-model-name\"\n",
    ")\n"
   ],
   "id": "a1255364ac72dfc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
