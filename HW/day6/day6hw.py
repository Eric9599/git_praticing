import os
import uuid
import torch
import requests
import pandas as pd
import gc
from typing import List
from tqdm import tqdm
from openai import OpenAI
from qdrant_client import QdrantClient, models
from transformers import AutoTokenizer, AutoModelForCausalLM
from langchain_core.embeddings import Embeddings

# --- 0. è¨­å®šç’°å¢ƒè®Šæ•¸ (è§£æ±º MPS è¨˜æ†¶é«”é™åˆ¶) ---
# é€™è¡Œå¿…é ˆåœ¨ import torch ä¹‹å‰æˆ–è€…æ˜¯ç¨‹å¼æœ€é–‹é ­è¨­å®š
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"

# --- 1. è¨­å®šèˆ‡å¸¸æ•¸ ---
EMBEDDING_API_URL = "http://ws-04.wade0426.me/embed"

TEXT_PATH = "HW/qa_data.txt"
PREDICT_INPUT = "HW/day6_HW_questions.csv.xlsx"
PREDICT_OUTPUT = "HW/day6_HW_questions_result.xlsx"

QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "rag_homework_day6_api"

LLM_BASE_URL = "https://ws-03.wade0426.me/v1/chat/completions"
LLM_MODEL_NAME = "/models/Llama-3_3-Nemotron-Super-49B-v1_5-NVFP4"
LLM_API_KEY = "day6hw"

# Reranker è¨­å®š
RERANKER_MODEL_PATH = os.path.expanduser("Qwen3-Reranker-0.6B")

# è¨­å®šé‹ç®—è£ç½®
if torch.cuda.is_available():
    device_obj = torch.device("cuda")
elif torch.backends.mps.is_available():
    device_obj = torch.device("mps")
else:
    device_obj = torch.device("cpu")

print(f"â³ ä½¿ç”¨è£ç½®: {device_obj}")


# --- 2. Embedding é¡åˆ¥ ---
class CustomAPIEmbeddings(Embeddings):
    def __init__(self, api_url):
        self.api_url = api_url

    def _call_api(self, texts: List[str]) -> List[List[float]]:
        data = {
            "texts": texts,
            "normalize": True,
            "batch_size": 32
        }

        try:
            response = requests.post(self.api_url, json=data, timeout=60)
            if response.status_code == 200:
                result = response.json()
                return result.get('embeddings', [])
            else:
                print(f"âŒ API Error Code: {response.status_code}")
                return []
        except Exception as e:
            print(f"âŒ API Exception: {e}")
            return []

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return self._call_api(texts)

    def embed_query(self, text: str) -> List[float]:
        results = self._call_api([text])
        if results and len(results) > 0:
            return results[0]
        return []


print(f"â³ åˆå§‹åŒ– Embedding API ({EMBEDDING_API_URL})...")
embedding_model = CustomAPIEmbeddings(EMBEDDING_API_URL)

# æ¸¬è©¦é€£ç·š
try:
    print("* æ¸¬è©¦ Embedding API é€£ç·š...")
    test_vec = embedding_model.embed_query("æ¸¬è©¦")
    if test_vec:
        print(f"âœ… API é€£ç·šæˆåŠŸï¼å‘é‡ç¶­åº¦: {len(test_vec)}")
    else:
        print("âŒ API é€£ç·šå¤±æ•—ï¼Œå›å‚³ç‚ºç©º")
        exit()
except Exception as e:
    print(f"âŒ API æ¸¬è©¦ç™¼ç”Ÿä¾‹å¤–éŒ¯èª¤: {e}")
    exit()


# --- 3. LLM Client ---
class SimpleLLMClient:
    def __init__(self, base_url, model_name, api_key):
        self.client = OpenAI(base_url=base_url, api_key=api_key)
        self.model_name = model_name

    def generate(self, prompt: str) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"LLM Error: {e}")
            return "Error generating response."


print("â³ åˆå§‹åŒ– LLM Client...")
llm_client = SimpleLLMClient(LLM_BASE_URL, LLM_MODEL_NAME, LLM_API_KEY)

# --- 4. Qdrant åˆå§‹åŒ– ---
client = QdrantClient(url=QDRANT_URL)


def simple_text_splitter(text, chunk_size=500, chunk_overlap=50):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start += (chunk_size - chunk_overlap)
    return chunks


def init_qdrant_collection():
    """æª¢æŸ¥ä¸¦å»ºç«‹ Qdrant Collection"""

    print(f"* æ­£åœ¨é‡ç½®é›†åˆ {COLLECTION_NAME}...")
    try:
        client.delete_collection(COLLECTION_NAME)
        print(f"ğŸ—‘ï¸ å·²åˆªé™¤èˆŠé›†åˆ {COLLECTION_NAME}")
    except Exception:
        pass

    print(f"* å»ºç«‹æ–° Collection: {COLLECTION_NAME}...")
    client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config={
            "dense": models.VectorParams(
                distance=models.Distance.COSINE,
                size=4096,
            ),
        },
        sparse_vectors_config={
            "sparse": models.SparseVectorParams(modifier=models.Modifier.IDF)
        },
    )

    if os.path.exists(TEXT_PATH):
        print(f"* è®€å– {TEXT_PATH} ä¸¦åŸ·è¡Œ Chunking...")

        with open(TEXT_PATH, "r", encoding="utf-8") as f:
            full_text = f.read()

        documents = simple_text_splitter(full_text, chunk_size=500, chunk_overlap=50)
        print(f"* åŸå§‹æ–‡æœ¬å·²åˆ‡åˆ†ç‚º {len(documents)} å€‹ Chunks")

        print("* è¨ˆç®— Embeddings...")
        doc_embeddings = embedding_model.embed_documents(documents)

        if not doc_embeddings:
            print("âŒ Embedding è¨ˆç®—å¤±æ•—")
            return

        points = [
            models.PointStruct(
                id=uuid.uuid4().hex,
                vector={
                    "dense": embedding,
                    "sparse": models.Document(text=doc, model="Qdrant/bm25"),
                },
                payload={"text": doc},
            )
            for doc, embedding in zip(documents, doc_embeddings)
        ]

        batch_size = 50
        print(f"â³ é–‹å§‹å¯«å…¥ {len(points)} ç­†è³‡æ–™è‡³ Qdrant...")
        for i in tqdm(range(0, len(points), batch_size), desc="Upserting"):
            client.upsert(
                collection_name=COLLECTION_NAME,
                points=points[i:i + batch_size]
            )
        print("âœ… è³‡æ–™å¯«å…¥å®Œæˆ")
    else:
        print(f"âš ï¸ æ‰¾ä¸åˆ° {TEXT_PATH}ï¼Œè·³éè³‡æ–™å¯«å…¥ã€‚")


init_qdrant_collection()

# --- 5. Reranker æ¨¡å‹è¼‰å…¥ ---
print("â³ è¼‰å…¥ Reranker æ¨¡å‹...")
reranker_tokenizer = AutoTokenizer.from_pretrained(
    RERANKER_MODEL_PATH, local_files_only=True, trust_remote_code=True
)
reranker_model = AutoModelForCausalLM.from_pretrained(
    RERANKER_MODEL_PATH, local_files_only=True, trust_remote_code=True
).to(device_obj).eval()

token_false_id = reranker_tokenizer.convert_tokens_to_ids("no")
token_true_id = reranker_tokenizer.convert_tokens_to_ids("yes")
max_reranker_length = 8192

prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
suffix = "<|im_end|>\n<|im_start|>assistant\n"
prefix_tokens = reranker_tokenizer.encode(prefix, add_special_tokens=False)
suffix_tokens = reranker_tokenizer.encode(suffix, add_special_tokens=False)


# ã€ä¿®æ­£é‡é»ã€‘ æ”¹ç‚ºåˆ†æ‰¹è™•ç† (Batch Processing)
def compute_rerank_scores(pairs, batch_size=4):
    """
    åˆ†æ‰¹è¨ˆç®— Reranker åˆ†æ•¸ï¼Œé¿å… MPS Out Of Memory
    batch_size: å»ºè­°è¨­å°ä¸€é» (ä¾‹å¦‚ 2 æˆ– 4)
    """
    all_scores = []

    # ä½¿ç”¨ tqdm é¡¯ç¤º Rerank é€²åº¦ (å¯é¸)
    # for i in range(0, len(pairs), batch_size):

    for i in range(0, len(pairs), batch_size):
        batch_pairs = pairs[i: i + batch_size]

        processed_inputs = []
        for pair in batch_pairs:
            pair_ids = reranker_tokenizer.encode(
                pair, add_special_tokens=False, truncation=True,
                max_length=max_reranker_length - len(prefix_tokens) - len(suffix_tokens)
            )
            full_ids = prefix_tokens + pair_ids + suffix_tokens
            processed_inputs.append(reranker_tokenizer.decode(full_ids))

        inputs = reranker_tokenizer(
            processed_inputs, padding=True, truncation=True, return_tensors="pt", max_length=max_reranker_length
        )

        # ç§»å‹•åˆ° GPU
        for key in inputs:
            inputs[key] = inputs[key].to(device_obj)

        with torch.no_grad():
            logits = reranker_model(**inputs).logits[:, -1, :]
            scores = logits[:, token_true_id].exp().tolist()
            all_scores.extend(scores)

        # æ¸…ç† GPU è¨˜æ†¶é«”
        del inputs, logits, scores
        if device_obj.type == "mps":
            torch.mps.empty_cache()
        elif device_obj.type == "cuda":
            torch.cuda.empty_cache()
        gc.collect()

    return all_scores


def rerank_documents(query, documents):
    if not documents: return []

    formatted_pairs = [
        f"<Instruct>: æ ¹æ“šæŸ¥è©¢æª¢ç´¢ç›¸é—œæ–‡ä»¶\n<Query>: {query}\n<Document>: {doc}"
        for doc in documents
    ]

    scores = compute_rerank_scores(formatted_pairs, batch_size=4)

    doc_scores = list(zip(documents, scores))
    doc_scores.sort(key=lambda x: x[1], reverse=True)
    return doc_scores


# --- 6. æ ¸å¿ƒæµç¨‹å‡½æ•¸ ---

def query_rewrite(query: str) -> str:
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹æœå°‹å¼•æ“å„ªåŒ–å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹ä½¿ç”¨è€…çš„å•é¡Œæ”¹å¯«ç‚ºæ›´ç²¾ç¢ºã€é©åˆåšèªç¾©æª¢ç´¢çš„é—œéµå­—æŸ¥è©¢ã€‚
    ä¿ç•™æ ¸å¿ƒæ„åœ–ï¼Œå»é™¤è´…è©ï¼Œä¸¦é‡å°è‡ªä¾†æ°´å…¬å¸ç›¸é—œæ¥­å‹™é€²è¡Œå„ªåŒ–ã€‚
    åªè¼¸å‡ºæ”¹å¯«å¾Œçš„å¥å­ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡‹ã€‚

    ä½¿ç”¨è€…å•é¡Œ: {query}
    æ”¹å¯«å¾ŒæŸ¥è©¢:
    """
    rewritten = llm_client.generate(prompt).strip()
    return rewritten


def hybrid_search_with_rerank(query: str, initial_limit=20, final_limit=3):
    query_vec = embedding_model.embed_query(query)

    try:
        response = client.query_points(
            collection_name=COLLECTION_NAME,
            prefetch=[
                models.Prefetch(
                    query=models.Document(text=query, model="Qdrant/bm25"),
                    using="sparse",
                    limit=initial_limit,
                ),
                models.Prefetch(
                    query=query_vec,
                    using="dense",
                    limit=initial_limit,
                ),
            ],
            query=models.FusionQuery(fusion=models.Fusion.RRF),
            limit=initial_limit,
        )
        candidate_docs = [point.payload["text"] for point in response.points]
    except Exception as e:
        print(f"Search Error: {e}")
        return []

    if not candidate_docs:
        return []

    # é€™è£¡é€²è¡Œ Rerank
    top_results = rerank_documents(query, candidate_docs)[:final_limit]
    return top_results


def main():
    print(f"ğŸ“‚ è®€å– Excel: {PREDICT_INPUT}")
    if not os.path.exists(PREDICT_INPUT):
        print("âŒ æª”æ¡ˆä¸å­˜åœ¨")
        return

    df = pd.read_excel(PREDICT_INPUT)

    # åˆå§‹åŒ– DataFrame çš„æ¬„ä½
    if 'q_id' not in df.columns: df['q_id'] = None
    if 'answer' not in df.columns: df['answer'] = None

    df['q_id'] = df['q_id'].astype('object')
    df['answer'] = df['answer'].astype('object')

    # ã€æ–°å¢ã€‘ç”¨ä¾†æš«å­˜ Ground Truth (Context) çš„åˆ—è¡¨
    ground_truth_list = []

    print("* é–‹å§‹è™•ç†å•é¡Œ...")

    for index, row in tqdm(df.iterrows(), total=df.shape[0]):
        original_question = str(row['questions'])

        # ç”¢ç”Ÿ ID
        current_uuid = str(uuid.uuid4())

        # 1. RAG æª¢ç´¢
        refined_query = query_rewrite(original_question)
        search_results = hybrid_search_with_rerank(refined_query)
        retrieval_context = [doc for doc, score in search_results]

        # è½‰æˆå­—ä¸²ä¾› Prompt ä½¿ç”¨
        context_str = "\n".join(retrieval_context)

        ground_truth_list.append({
            "q_id": current_uuid,
            "questions": original_question,
            "contexts": retrieval_context,  # DeepEval éœ€è¦ list
            "ground_truth": ""  # é ç•™æ¬„ä½ï¼ŒDeepEval çš„ Recall éœ€è¦é€™å€‹ (æ¨™æº–ç­”æ¡ˆ)
        })

        # 3. ç”Ÿæˆ Answer
        qa_prompt = f"""
        ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è‡ªä¾†æ°´å…¬å¸å®¢æœåŠ©æ‰‹ã€‚è«‹æ ¹æ“šã€åƒè€ƒè³‡æ–™ã€‘å›ç­”ä½¿ç”¨è€…çš„ã€å•é¡Œã€‘ã€‚

        è¦ç¯„ï¼š
        1. ç­”æ¡ˆå¿…é ˆåŸºæ–¼åƒè€ƒè³‡æ–™ï¼Œä¸è¦ç·¨é€ ã€‚
        2. å¦‚æœåƒè€ƒè³‡æ–™ä¸è¶³ä»¥å›ç­”ï¼Œè«‹å›ç­”ã€Œç›®å‰è³‡è¨Šä¸è¶³ï¼Œå»ºè­°è¯ç¹«å®¢æœã€ã€‚
        3. èªæ°£è¦ªåˆ‡ã€å°ˆæ¥­ã€‚

        ã€åƒè€ƒè³‡æ–™ã€‘ï¼š
        {context_str}

        ã€å•é¡Œã€‘ï¼š{original_question}

        ã€å›ç­”ã€‘ï¼š
        """
        answer = llm_client.generate(qa_prompt)

        # 4. å°‡ Answer å¯«å›åŸæœ¬çš„ DataFrame
        df.at[index, 'q_id'] = current_uuid
        df.at[index, 'answer'] = answer

    # --- è¿´åœˆçµæŸï¼Œé–‹å§‹å­˜æª” ---

    # æª”æ¡ˆ 1ï¼šå­˜ Answer çš„ Excel
    df.to_excel(PREDICT_OUTPUT, index=False)
    print(f"âœ… Answer è™•ç†å®Œæˆï¼çµæœå·²å„²å­˜è‡³: {PREDICT_OUTPUT}")

    # æª”æ¡ˆ 2ï¼šå­˜ Ground Truth (Context) çš„ CSV
    gt_df = pd.DataFrame(ground_truth_list)

    # å­˜æˆ CSV (å»ºè­°ç”¨ utf-8-sig ä»¥å…ä¸­æ–‡äº‚ç¢¼)
    gt_df.to_csv("ground_truth.csv", index=False, encoding='utf-8-sig')
    print(f"âœ… Ground Truth (Context) å·²å„²å­˜è‡³: ground_truth.csv")


if __name__ == "__main__":
    main()
