import os
import requests
import pandas as pd
import torch
from uuid import uuid4
from qdrant_client import QdrantClient, models
from qdrant_client.http.models import VectorParams, Distance, PointStruct
from semantic_text_splitter import TextSplitter
from langchain_openai import ChatOpenAI
from transformers import AutoTokenizer, AutoModelForCausalLM

# --- 1. è¨­å®šå€ ---
EMBEDDING_API_URL = "http://ws-04.wade0426.me/embed"
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "data_collection_final_v2"  # æ”¹åä»¥ç¢ºä¿ä¹¾æ·¨
MODEL_PATH = os.path.expanduser("./Qwen3-Reranker-0.6B")
TEXT_PATH = "./CW"
TEXT_LIST = ["data_01.txt", "data_02.txt", "data_03.txt", "data_04.txt", "data_05.txt"]
CSV_INPUT_PATH = "./CW/Re_Write_questions.csv"
CSV_OUTPUT_PATH = "Completed_Questions_with_Sources_Final.csv"

# Reranker è¨­å®š
MAX_RERANKER_LENGTH = 8192
DEVICE = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else "cpu"
print(f"ğŸ–¥ï¸ é‹è¡Œè£ç½®: {DEVICE}")


# --- 2. è‡ªå‹•åµæ¸¬èˆ‡ Embedding å‡½æ•¸ (æœ€é—œéµçš„ä¿®æ­£) ---

def get_embedding(texts):
    """
    ä¿®æ­£å¾Œçš„ Embedding å‡½æ•¸ï¼Œé‡å° ws-04 API æ ¼å¼
    Request: {"texts": [...]}
    Response: {"embeddings": [[...]]}
    """
    if isinstance(texts, str):
        texts = [texts]

    # ä¿®æ­£ 1: Key å¿…é ˆæ˜¯ 'texts'
    payload = {"texts": texts}

    try:
        response = requests.post(EMBEDDING_API_URL, json=payload, timeout=30)

        # å¦‚æœä¸æ˜¯ 200ï¼Œå°å‡ºè©³ç´°éŒ¯èª¤
        if response.status_code != 200:
            print(f"âŒ API å ±éŒ¯ ({response.status_code}): {response.text}")
            return []

        result = response.json()

        # ä¿®æ­£ 2: å„ªå…ˆæŠ“å– 'embeddings'
        if "embeddings" in result:
            return result["embeddings"]
        elif "data" in result:  # ç›¸å®¹æ€§ä¿ç•™
            return [item["embedding"] for item in result["data"]]
        else:
            print(f"âŒ API å›å‚³æ ¼å¼ä¸ç¬¦: {result.keys()}")
            return []
    except Exception as e:
        print(f"âŒ API é€£ç·šå¤±æ•—: {e}")
        return []


def check_and_set_dimension():
    """è‡ªå‹•åµæ¸¬ API çš„å‘é‡ç¶­åº¦ï¼Œé¿å…è¨­å®šéŒ¯èª¤"""
    print("ğŸ•µï¸â€â™‚ï¸ æ­£åœ¨åµæ¸¬ Embedding API çš„å‘é‡ç¶­åº¦...")
    vecs = get_embedding(["test"])
    if vecs and len(vecs) > 0:
        dim = len(vecs[0])
        print(f"âœ… åµæ¸¬æˆåŠŸï¼å‘é‡ç¶­åº¦ç‚º: {dim}")
        return dim
    else:
        print("âŒ ç„¡æ³•åµæ¸¬ç¶­åº¦ï¼Œå°‡ä½¿ç”¨é è¨­å€¼ 1536 (å¯èƒ½æœƒå¤±æ•—)")
        return 1536


# è‡ªå‹•è¨­å®šæ­£ç¢ºçš„ç¶­åº¦
VECTOR_SIZE = check_and_set_dimension()

# --- 3. åˆå§‹åŒ– Qdrant èˆ‡ LLM ---
client = QdrantClient(url=QDRANT_URL)
llm = ChatOpenAI(
    base_url="https://ws-02.wade0426.me/v1",
    api_key="day6hw",
    model="gpt-4o",
    temperature=0
)

# --- 4. åˆå§‹åŒ– Reranker æ¨¡å‹ ---
reranker_model = None
reranker_tokenizer = None

if os.path.exists(MODEL_PATH):
    try:
        print(f"â³ æ­£åœ¨è¼‰å…¥ Reranker æ¨¡å‹ ({MODEL_PATH})...")
        reranker_tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH, local_files_only=True, trust_remote_code=True
        )
        reranker_model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH, local_files_only=True, trust_remote_code=True
        ).to(DEVICE).eval()

        token_false_id = reranker_tokenizer.convert_tokens_to_ids("no")
        token_true_id = reranker_tokenizer.convert_tokens_to_ids("yes")

        # Prompt æ¨¡æ¿
        PREFIX = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
        SUFFIX = "<|im_end|>\n<|im_start|>assistant\n"
        PREFIX_TOKENS = reranker_tokenizer.encode(PREFIX, add_special_tokens=False)
        SUFFIX_TOKENS = reranker_tokenizer.encode(SUFFIX, add_special_tokens=False)
        print("âœ… Reranker æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")
    except Exception as e:
        print(f"âš ï¸ Reranker æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
else:
    print(f"âš ï¸ æ‰¾ä¸åˆ° Reranker è·¯å¾‘ï¼Œå°‡è·³éã€‚")


# --- 5. Reranker é‚è¼¯ ---

def format_instruction(instruction, query, doc):
    if instruction is None: instruction = 'æ ¹æ“šæŸ¥è©¢æª¢ç´¢ç›¸é—œæ–‡ä»¶'
    return "<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}".format(
        instruction=instruction, query=query, doc=doc
    )


def calculate_rerank_scores(pairs):
    if not reranker_model or not reranker_tokenizer: return [0.0] * len(pairs)

    processed_pairs = []
    for pair in pairs:
        pair_ids = reranker_tokenizer.encode(
            pair, add_special_tokens=False, truncation=True,
            max_length=MAX_RERANKER_LENGTH - len(PREFIX_TOKENS) - len(SUFFIX_TOKENS)
        )
        full_ids = PREFIX_TOKENS + pair_ids + SUFFIX_TOKENS
        processed_pairs.append(reranker_tokenizer.decode(full_ids))

    inputs = reranker_tokenizer(
        processed_pairs, padding=True, truncation=True, return_tensors="pt", max_length=MAX_RERANKER_LENGTH
    )
    for key in inputs: inputs[key] = inputs[key].to(DEVICE)

    with torch.no_grad():
        batch_scores = reranker_model(**inputs).logits[:, -1, :]
        true_vector = batch_scores[:, token_true_id]
        false_vector = batch_scores[:, token_false_id]
        batch_scores = torch.stack([false_vector, true_vector], dim=1)
        batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)
        return batch_scores[:, 1].exp().tolist()


def rerank_documents(query, documents, task_instruction=None):
    if not reranker_model: return [(doc, 1.0) for doc in documents]  # Fallback

    pairs = [format_instruction(task_instruction, query, doc) for doc in documents]
    scores = calculate_rerank_scores(pairs)
    doc_scores = list(zip(documents, scores))
    doc_scores.sort(key=lambda x: x[1], reverse=True)
    return doc_scores


# --- 6. æª¢ç´¢ Pipeline ---

def rewrite_query(original_question):
    try:
        sys_prompt = "ä½ æ˜¯ä¸€å€‹æœå°‹å°ˆå®¶ã€‚è«‹å°‡ä½¿ç”¨è€…çš„å•é¡Œã€Œé‡å¯«ã€ç‚ºç²¾ç¢ºçš„æœå°‹æŸ¥è©¢ï¼Œå»é™¤å£èªè©ï¼Œè£œå……é—œéµå­—ï¼Œç›´æ¥è¼¸å‡ºçµæœã€‚"
        response = llm.invoke([("system", sys_prompt), ("user", original_question)])
        return response.content.strip()
    except:
        return original_question


def retrieve_pipeline(query, top_k=3, initial_k=20):
    # 1. Embedding
    query_vecs = get_embedding([query])
    if not query_vecs: return "", ""
    query_vector = query_vecs[0]

    # 2. Qdrant Search
    try:
        results = client.query_points(
            collection_name=COLLECTION_NAME, query=query_vector, limit=initial_k
        ).points
    except:
        results = client.search(
            collection_name=COLLECTION_NAME, query_vector=query_vector, limit=initial_k
        )

    if not results: return "", ""

    # 3. Data Prep for Reranker
    docs_map = {}
    docs_text = []
    for hit in results:
        payload = hit.payload
        text = payload.get("text", "")
        file_name = payload.get("file_name", "Unknown")
        if text not in docs_map:
            docs_text.append(text)
            docs_map[text] = file_name

    # 4. Rerank
    ranked_results = rerank_documents(query, docs_text)
    final_docs = ranked_results[:top_k]

    # 5. Format
    context_segments = []
    source_set = set()
    for text, score in final_docs:
        fname = docs_map.get(text, "Unknown")
        source_set.add(fname)
        context_segments.append(f"ã€ä¾†æºï¼š{fname} | åˆ†æ•¸ï¼š{score:.4f}ã€‘\n{text}")

    return "\n\n".join(context_segments), ", ".join(list(source_set))


def answer_question_pipeline(original_question):
    rewritten = rewrite_query(original_question)
    print(f"ğŸ” Rewrite: {rewritten}")
    context, sources = retrieve_pipeline(rewritten)

    if not context: return "æŸ¥ç„¡ç›¸é—œè³‡æ–™ã€‚", ""

    prompt = f"""
    è«‹æ ¹æ“šã€èƒŒæ™¯è³‡æ–™ã€‘å›ç­”å•é¡Œã€‚å¿…é ˆå¼•ç”¨ä¾†æºã€‚è‹¥è³‡æ–™ä¸è¶³è«‹å›ç­”ä¸çŸ¥é“ã€‚

    ã€èƒŒæ™¯è³‡æ–™ã€‘ï¼š
    {context}

    ã€å•é¡Œã€‘ï¼š
    {original_question}
    """
    try:
        ans = llm.invoke(prompt).content
        return ans, sources
    except Exception as e:
        return f"Error: {e}", ""


# --- 7. ä¸»ç¨‹å¼åŸ·è¡Œ (å¼·åˆ¶é‡ç½®è³‡æ–™åº«) ---

print(f"ğŸ’¥ æ­£åœ¨é‡å»º Collection: {COLLECTION_NAME} (ç¶­åº¦: {VECTOR_SIZE})...")
try:
    client.delete_collection(COLLECTION_NAME)
    print("ğŸ—‘ï¸ èˆŠè³‡æ–™å·²åˆªé™¤ã€‚")
except:
    pass

client.create_collection(
    collection_name=COLLECTION_NAME,
    vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)
)

print("ğŸ“‚ é–‹å§‹ä¸Šå‚³è³‡æ–™...")
splitter = TextSplitter((200, 1000))
total_uploaded = 0

for file_name in TEXT_LIST:
    path = os.path.join(TEXT_PATH, file_name)
    if not os.path.exists(path):
        print(f"âš ï¸ æ‰¾ä¸åˆ°æª”æ¡ˆ: {path}")
        continue

    with open(path, "r", encoding="utf-8") as f:
        content = f.read()
    if not content: continue

    chunks = splitter.chunks(content)
    # é€™è£¡æœƒå‘¼å«ä¿®æ­£å¾Œçš„ get_embedding
    vectors = get_embedding(chunks)

    if vectors and len(vectors) == len(chunks):
        points = [
            PointStruct(
                id=str(uuid4()),
                vector=v,
                payload={"file_name": file_name, "text": c}
            ) for c, v in zip(chunks, vectors)
        ]
        client.upsert(collection_name=COLLECTION_NAME, points=points)
        total_uploaded += len(points)
        print(f"   âœ… {file_name}: æˆåŠŸä¸Šå‚³ {len(points)} ç­†ã€‚")
    else:
        print(f"   âŒ {file_name}: å‘é‡ç”Ÿæˆå¤±æ•— (ç¶­åº¦å¯èƒ½ä¸ç¬¦æˆ–APIéŒ¯èª¤)")

print(f"ğŸ è³‡æ–™åº«æº–å‚™å®Œæˆï¼Œå…± {total_uploaded} ç­†è³‡æ–™ã€‚")

# --- 8. åŸ·è¡Œ CSV ---
if os.path.exists(CSV_INPUT_PATH) and total_uploaded > 0:
    print("\nğŸš€ é–‹å§‹å›ç­” CSV å•é¡Œ...")
    df = pd.read_csv(CSV_INPUT_PATH)
    ans_list, src_list = [], []

    for idx, row in df.iterrows():
        q = row.get('questions', row.get('question'))
        print(f"ğŸ“ ({idx + 1}) {q}")
        a, s = answer_question_pipeline(q)
        ans_list.append(a)
        src_list.append(s)

    df['answer'] = ans_list
    df['source'] = src_list
    df.to_csv(CSV_OUTPUT_PATH, index=False, encoding="utf-8-sig")
    print(f"âœ¨ å®Œæˆï¼çµæœå·²å­˜è‡³ {CSV_OUTPUT_PATH}")
else:
    print(f"âš ï¸ ç„¡æ³•åŸ·è¡Œå•ç­”ï¼šæ‰¾ä¸åˆ° CSV æˆ–è³‡æ–™åº«ç‚ºç©ºã€‚")